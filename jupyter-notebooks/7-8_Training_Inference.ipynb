{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"},"colab":{"name":"6_Training_Inference.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"sound-volume"},"source":["# Lesson 6 - Training & Inference\n","- 6강에서는 모델을 학습하고 추론하는 방법에 대해 알아보았습니다.\n","- 이번 실습 자료에서는 다양한 Loss, Optimizer, Scheduler를 활용하는 방법을 알아봅니다.\n","- 또한, Checkpoint, Early Stopping과 같은 학습을 도와주는 Callback 방법을 알아봅니다.\n","- 그리고 Graident Accumulation 방법을 활용하여 학습을 진행해봅니다.\n"],"id":"sound-volume"},{"cell_type":"markdown","metadata":{"id":"gentle-appeal"},"source":["## 1. Loss\n","- Image Classification에 사용되는 다양한 loss 함수들이 존재합니다. 각 loss 함수는 목적이 있고 풀고자 하는 문제에 맞게 적용을 해야합니다.\n","- Cross Entropy Loss는 두 분포간의 불확실성을 최소화 하는 목적을 가진 분류에 사용되는 일반적인 손실함수입니다.\n","- Focal Loss는 Imbalanced Data 문제를 해결하기 위한 손실함수입니다. [참고](https://arxiv.org/pdf/1708.02002.pdf)\n","- Label Smoothing은 학습 데이터의 representation을 더 잘나타내는데 도움을 줍니다. [참고](https://arxiv.org/pdf/1906.02629.pdf)\n","- F1 Loss는 F1 score 향상을 목적으로 하는 손실함수입니다."],"id":"gentle-appeal"},{"cell_type":"markdown","metadata":{"id":"90bf3a7f"},"source":["###### 베이스라인 코드 train.py 파일 중 Loss 관련 부분.\n","- default 로 cross_entropy로 설정되어 있지만 FocalLoss, LabelSmoothingLoss, F1Loss로 바꿔서 적용해 봅시다!"],"id":"90bf3a7f"},{"cell_type":"code","metadata":{"id":"3fca816b"},"source":["# -- loss & metric\n","criterion = create_criterion(args.criterion)  # default: cross_entropy"],"id":"3fca816b","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f8579fef"},"source":["###### 베이스라인 코드 loss.py 파일.\n","- FocalLoss, LabelSmoothingLoss, F1Loss, cross_entropy 중에 원하는 Loss 를 설정하여 학습시킬 수 있습니다. \n","또한 직접 커스터마이징한 Loss를 추가해서 적용해 볼수 있습니다! (아래쪽 Reference를 참고하세요)"],"id":"f8579fef"},{"cell_type":"code","metadata":{"tags":[],"id":"closed-launch"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","# https://discuss.pytorch.org/t/is-this-a-correct-implementation-for-focal-loss-in-pytorch/43327/8\n","class FocalLoss(nn.Module):\n","    def __init__(self, weight=None,\n","                 gamma=2., reduction='mean'):\n","        nn.Module.__init__(self)\n","        self.weight = weight\n","        self.gamma = gamma\n","        self.reduction = reduction\n","\n","    def forward(self, input_tensor, target_tensor):\n","        log_prob = F.log_softmax(input_tensor, dim=-1)\n","        prob = torch.exp(log_prob)\n","        return F.nll_loss(\n","            ((1 - prob) ** self.gamma) * log_prob,\n","            target_tensor,\n","            weight=self.weight,\n","            reduction=self.reduction\n","        )"],"id":"closed-launch","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"id":"filled-configuration"},"source":["class LabelSmoothingLoss(nn.Module):\n","    def __init__(self, classes=3, smoothing=0.0, dim=-1):\n","        super(LabelSmoothingLoss, self).__init__()\n","        self.confidence = 1.0 - smoothing\n","        self.smoothing = smoothing\n","        self.cls = classes\n","        self.dim = dim\n","\n","    def forward(self, pred, target):\n","        pred = pred.log_softmax(dim=self.dim)\n","        with torch.no_grad():\n","            true_dist = torch.zeros_like(pred)\n","            true_dist.fill_(self.smoothing / (self.cls - 1))\n","            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n","        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))"],"id":"filled-configuration","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"id":"informative-banks"},"source":["# https://gist.github.com/SuperShinyEyes/dcc68a08ff8b615442e3bc6a9b55a354\n","class F1Loss(nn.Module):\n","    def __init__(self, classes=3, epsilon=1e-7):\n","        super().__init__()\n","        self.classes = classes\n","        self.epsilon = epsilon\n","    def forward(self, y_pred, y_true):\n","        assert y_pred.ndim == 2\n","        assert y_true.ndim == 1\n","        y_true = F.one_hot(y_true, self.classes).to(torch.float32)\n","        y_pred = F.softmax(y_pred, dim=1)\n","\n","        tp = (y_true * y_pred).sum(dim=0).to(torch.float32)\n","        tn = ((1 - y_true) * (1 - y_pred)).sum(dim=0).to(torch.float32)\n","        fp = ((1 - y_true) * y_pred).sum(dim=0).to(torch.float32)\n","        fn = (y_true * (1 - y_pred)).sum(dim=0).to(torch.float32)\n","\n","        precision = tp / (tp + fp + self.epsilon)\n","        recall = tp / (tp + fn + self.epsilon)\n","\n","        f1 = 2 * (precision * recall) / (precision + recall + self.epsilon)\n","        f1 = f1.clamp(min=self.epsilon, max=1 - self.epsilon)\n","        return 1 - f1.mean()"],"id":"informative-banks","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"id":"desirable-accounting"},"source":["_criterion_entrypoints = {\n","    'cross_entropy': nn.CrossEntropyLoss,\n","    'focal': FocalLoss,\n","    'label_smoothing': LabelSmoothingLoss,\n","    'f1': F1Loss\n","}"],"id":"desirable-accounting","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gothic-picking"},"source":["## 2. Optimizer\n","- 파이토치는 코드를 간단히 수정하여 다양한 optimizer를 사용할 수 있습니다.\n","- 이번 실습 자료에서는 Adam 과 SGD Optimizer 를 활용하는 방법을 알아봅니다.\n","\n","\n","### 2.1 예제를 통해 그래프로 이해하기\n","- 예제를 통해 Optimizer 따라 Loss 값이 변하는 추이를 그래프를 통해서 이해해봅시다.\n","- 비교할 Optimizer 는 베이스라인 코드에서 default 로 사용된 SGD와 Adam Optimizer 입니다."],"id":"gothic-picking"},{"cell_type":"code","metadata":{"id":"b9c1313c"},"source":["import torch\n","import torch.utils.data as Data\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","import matplotlib.pyplot as plt\n","\n","import torch.nn as nn\n","\n","# -- 기본 모델 정의.\n","class Net(torch.nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.hidden = torch.nn.Linear(1, 20)   # 은닉층\n","        self.predict = torch.nn.Linear(20, 1)   # 결과층\n","\n","    def forward(self, x):\n","        x = F.relu(self.hidden(x))      # activation function for hidden layer\n","        x = self.predict(x)             # 결과.\n","        return x\n"],"id":"b9c1313c","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0f8b3f99"},"source":["# -- Hyperparameter 정의.\n","LR = 0.01  #Learning rate\n","BATCH_SIZE = 32 \n","EPOCH = 15 \n","\n","x = torch.unsqueeze(torch.linspace(-1,1,1000),dim=1)\n","y = x.pow(2) + 0.1*torch.normal(torch.zeros(x.size()))\n","\n","# -- 데이터셋 생성.\n","torch_dataset = Data.TensorDataset(x,y)\n","loader = Data.DataLoader(\n","    dataset=torch_dataset,\n","    batch_size=BATCH_SIZE,\n","    shuffle=True,\n","    num_workers=2\n",")"],"id":"0f8b3f99","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1d863901"},"source":["# -- 각 optimizer를 사용한 신경망 정의.\n","net_SGD = Net()\n","net_Adam = Net()\n","\n","# -- 각 신경망 리스트.\n","nets = [net_SGD, net_Adam]\n","\n","# -- 각 Optimizer 정의.\n","opt_SGD = torch.optim.SGD(net_SGD.parameters(), lr=LR) # SGD Optimizer\n","opt_Adam = torch.optim.Adam(net_Adam.parameters(), lr=LR, betas=(0.9,0.99)) # Adam Optimizer\n","\n","# -- 각 Optimizer 리스트.\n","optimizers = [opt_SGD, opt_Adam]\n","\n","# -- Loss 계산.\n","loss_func = torch.nn.MSELoss()\n","# -- Loss 기록.\n","losses_his = [[],[]]"],"id":"1d863901","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8cc5259c"},"source":["#-- Training \n","for epoch in range(EPOCH):\n","    print(\"[INFO] Training Epoch : {}\".format(epoch))\n","    for step, (batch_x,batch_y) in enumerate(loader):\n","        \n","        b_x = Variable(batch_x)\n","        b_y = Variable(batch_y)\n","                \n","        for net, opt, l_his in zip(nets, optimizers, losses_his):\n","            output = net(b_x) \n","            loss = loss_func(output, b_y) #각 신경망의 Loss 계산.\n","            \n","            opt.zero_grad() \n","            loss.backward() \n","            opt.step() \n","            \n","\n","            l_his.append(loss.item()) \n","print(\"[INFO] Training ALL DONE!!!\")"],"id":"8cc5259c","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0ef0bbe5"},"source":["#-- 그래프로 시각화.\n","labels = ['SGD', 'Adam']\n","colors = ['darkred', 'royalblue']\n","for i, l_his in enumerate(losses_his):\n","    plt.plot(l_his, label=labels[i], color=colors[i])\n","plt.legend(loc='best')\n","plt.xlabel('Steps')\n","plt.ylabel('Loss')\n","plt.ylim(0,0.25)  \n","plt.show()"],"id":"0ef0bbe5","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bb6e694b"},"source":["- 그래프를 통해 Adam 의 Loss 값이 SGD에서 더 낮은것을 확인할 수 있습니다! 실제로 베이스라인 코드에 적용해 볼까요?"],"id":"bb6e694b"},{"cell_type":"markdown","metadata":{"id":"3f70ca3a"},"source":["### 2.2 베이스라인 코드에 적용하며 이해하기\n","\n","- 이번엔 베이스라인 코드에서 SGD Optimizer를 먼저 적용해 보고 Adam Optimizer를 적용한 결과의 Loss와 Accuracy의 변화를 비교해 살펴보겠습니다.\n","- 결론 부터 말씀드리면 Adam(Adaptive moment estimation) Optimizer 는 다른 stochastic optimization methods(SGD, Adagrad, RMSProp)에 비해 optimizer 성능이 좋은 편입니다! 실제로 확인해볼까요? [참고](https://arxiv.org/pdf/1412.6980.pdf)\n"],"id":"3f70ca3a"},{"cell_type":"markdown","metadata":{"id":"2b5571a7"},"source":["- 다음은 베이스라인 코드중 train.py 파일의 일부분 입니다. \n","\n","###### Libraries & Configurations"],"id":"2b5571a7"},{"cell_type":"code","metadata":{"id":"ec5887c5"},"source":["import argparse\n","import glob\n","import json\n","import os\n","import random\n","import re\n","from importlib import import_module\n","from pathlib import Path\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import torch\n","from torch.optim.lr_scheduler import StepLR\n","from torch.utils.data import Subset, DataLoader\n","from torch.utils.tensorboard import SummaryWriter\n","\n","from dataset import MaskBaseDataset\n","from loss import create_criterion\n","\n","\n","def seed_everything(seed):\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","    np.random.seed(seed)\n","    random.seed(seed)\n","\n","\n","def get_lr(optimizer):\n","    for param_group in optimizer.param_groups:\n","        return param_group['lr']"],"id":"ec5887c5","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"340d2986"},"source":["###### Loss"],"id":"340d2986"},{"cell_type":"code","metadata":{"id":"a4878c25"},"source":["# -- loss & metric\n","criterion = create_criterion(args.criterion)  # default: cross_entropy"],"id":"a4878c25","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"05c831b4"},"source":["- 아래쪽의 Optimizer를 적용하는 부분은 다음과 같이 간단히 한줄로 적용할 수도 있습니다.\n","###### SGD optimizer\n","    optimizer = SGD(model.parameters(), lr=lr, weight_decay=5e-4)\n","###### Adam optimizer\n","    optimizer = Adam(model.parameters(), lr=lr, weight_decay=5e-4)"],"id":"05c831b4"},{"cell_type":"markdown","metadata":{"id":"e008b57e"},"source":["###### Optimizer"],"id":"e008b57e"},{"cell_type":"code","metadata":{"id":"9d21259b"},"source":["opt_module = getattr(import_module(\"torch.optim\"), args.optimizer)  # default: SGD\n","optimizer = opt_module(\n","    filter(lambda p: p.requires_grad, model.parameters()),\n","    lr=args.lr,\n","    weight_decay=5e-4\n",")"],"id":"9d21259b","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"641da963"},"source":["###### Scheduler"],"id":"641da963"},{"cell_type":"code","metadata":{"id":"bcf2e49c"},"source":["scheduler = StepLR(optimizer, args.lr_decay_step, gamma=0.5)"],"id":"bcf2e49c","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5f642b9d"},"source":["- SGD Optimizer 결과."],"id":"5f642b9d"},{"cell_type":"markdown","metadata":{"id":"66195987"},"source":["Epoch[98/100](20/241) || training loss 2.278 || training accuracy 26.56% || lr 6.25e-05\n","\n","Epoch[98/100](40/241) || training loss 2.261 || training accuracy 28.44% || lr 6.25e-05\n","\n","Epoch[98/100](60/241) || training loss 2.258 || training accuracy 28.75% || lr 6.25e-05\n","\n","Epoch[98/100](80/241) || training loss 2.286 || training accuracy 28.83% || lr 6.25e-05\n","\n","Epoch[98/100](100/241) || training loss  2.3 || training accuracy 28.05% || lr 6.25e-05\n","\n","Epoch[98/100](120/241) || training loss 2.298 || training accuracy 26.48% || lr 6.25e-05\n","\n","Epoch[98/100](140/241) || training loss 2.232 || training accuracy 28.75% || lr 6.25e-05\n","\n","Epoch[98/100](160/241) || training loss 2.338 || training accuracy 26.56% || lr 6.25e-05\n","\n","Epoch[98/100](180/241) || training loss 2.206 || training accuracy 29.61% || lr 6.25e-05\n","\n","Epoch[98/100](200/241) || training loss  2.3 || training accuracy 25.86% || lr 6.25e-05\n","\n","Epoch[98/100](220/241) || training loss 2.269 || training accuracy 27.89% || lr 6.25e-05\n","\n","Epoch[98/100](240/241) || training loss 2.294 || training accuracy 27.97% || lr 6.25e-05\n","\n","Calculating validation results...\n","[Val] acc : 24.71%, loss:  2.3 || best acc : 24.80%, best loss:  2.3\n","\n","Epoch[99/100](20/241) || training loss 2.258 || training accuracy 28.98% || lr 6.25e-05\n","\n","Epoch[99/100](40/241) || training loss 2.273 || training accuracy 27.66% || lr 6.25e-05\n","\n","Epoch[99/100](60/241) || training loss 2.299 || training accuracy 25.47% || lr 6.25e-05\n","\n","Epoch[99/100](80/241) || training loss 2.325 || training accuracy 26.02% || lr 6.25e-05\n","\n","Epoch[99/100](100/241) || training loss 2.27 || training accuracy 26.25% || lr 6.25e-05\n","\n","Epoch[99/100](120/241) || training loss 2.237 || training accuracy 29.38% || lr 6.25e-05\n","\n","Epoch[99/100](140/241) || training loss 2.281 || training accuracy 27.89% || lr 6.25e-05\n","\n","Epoch[99/100](160/241) || training loss 2.264 || training accuracy 28.91% || lr 6.25e-05\n","\n","Epoch[99/100](180/241) || training loss 2.256 || training accuracy 30.16% || lr 6.25e-05\n","\n","Epoch[99/100](200/241) || training loss 2.304 || training accuracy 28.12% || lr 6.25e-05\n","\n","Epoch[99/100](220/241) || training loss 2.262 || training accuracy 29.38% || lr 6.25e-05\n","\n","Epoch[99/100](240/241) || training loss 2.282 || training accuracy 27.42% || lr 6.25e-05\n","\n","Calculating validation results...\n","[Val] acc : 24.80%, loss:  2.3 || best acc : 24.80%, best loss:  2.3"],"id":"66195987"},{"cell_type":"markdown","metadata":{"id":"b9df4610"},"source":["- Adam Optimizer 결과로, Optimizer 만 바꿨을 뿐인데 확연히 낮아진 Loss값과 향상된 Accuracy 를 확인할 수 있습니다"],"id":"b9df4610"},{"cell_type":"markdown","metadata":{"id":"ef82e7f0"},"source":["Epoch[98/100](20/241) || training loss 0.3912 || training accuracy 86.72% || lr 6.25e-05\n","\n","Epoch[98/100](40/241) || training loss 0.4169 || training accuracy 86.72% || lr 6.25e-05\n","\n","Epoch[98/100](60/241) || training loss 0.4019 || training accuracy 86.56% || lr 6.25e-05\n","\n","Epoch[98/100](80/241) || training loss 0.428 || training accuracy 85.70% || lr 6.25e-05\n","\n","Epoch[98/100](100/241) || training loss 0.4092 || training accuracy 85.78% || lr 6.25e-05\n","\n","Epoch[98/100](120/241) || training loss 0.4237 || training accuracy 86.33% || lr 6.25e-05\n","\n","Epoch[98/100](140/241) || training loss 0.4093 || training accuracy 86.25% || lr 6.25e-05\n","\n","Epoch[98/100](160/241) || training loss 0.3903 || training accuracy 87.19% || lr 6.25e-05\n","\n","Epoch[98/100](180/241) || training loss 0.4056 || training accuracy 86.64% || lr 6.25e-05\n","\n","Epoch[98/100](200/241) || training loss 0.408 || training accuracy 86.72% || lr 6.25e-05\n","\n","Epoch[98/100](220/241) || training loss 0.4367 || training accuracy 84.92% || lr 6.25e-05\n","\n","Epoch[98/100](240/241) || training loss 0.4363 || training accuracy 85.55% || lr 6.25e-05\n","\n","Calculating validation results...\n","[Val] acc : 68.12%, loss: 0.71 || best acc : 68.30%, best loss:  0.7\n","\n","Epoch[99/100](20/241) || training loss 0.3987 || training accuracy 87.11% || lr 6.25e-05\n","\n","Epoch[99/100](40/241) || training loss 0.4065 || training accuracy 87.89% || lr 6.25e-05\n","\n","Epoch[99/100](60/241) || training loss 0.4084 || training accuracy 86.25% || lr 6.25e-05\n","\n","Epoch[99/100](80/241) || training loss 0.4135 || training accuracy 86.56% || lr 6.25e-05\n","\n","Epoch[99/100](100/241) || training loss 0.4168 || training accuracy 85.78% || lr 6.25e-05\n","\n","Epoch[99/100](120/241) || training loss 0.3895 || training accuracy 86.48% || lr 6.25e-05\n","\n","Epoch[99/100](140/241) || training loss 0.4228 || training accuracy 85.62% || lr 6.25e-05\n","\n","Epoch[99/100](160/241) || training loss 0.4314 || training accuracy 85.86% || lr 6.25e-05\n","\n","Epoch[99/100](180/241) || training loss 0.4434 || training accuracy 84.92% || lr 6.25e-05\n","\n","Epoch[99/100](200/241) || training loss 0.4697 || training accuracy 84.53% || lr 6.25e-05\n","\n","Epoch[99/100](220/241) || training loss 0.3847 || training accuracy 87.73% || lr 6.25e-05\n","\n","Epoch[99/100](240/241) || training loss 0.4097 || training accuracy 85.86% || lr 6.25e-05\n","\n","Calculating validation results...\n","[Val] acc : 67.80%, loss:  0.7 || best acc : 68.30%, best loss:  0.7"],"id":"ef82e7f0"},{"cell_type":"markdown","metadata":{"id":"informative-merchandise"},"source":["## 3. Scheduler\n","- Scheduler은 optimizer의 learning rate를 동적으로 변경시키는 기능을 합니다.\n","- Optimizer과 Scheduler를 적절히 활용하면 모델이 좋은 성능으로 Fitting하는데 도움을 줍니다."],"id":"informative-merchandise"},{"cell_type":"code","metadata":{"id":"original-breath"},"source":["# -- scheduler: StepLR\n","# 지정된 step마다 learning rate를 감소시킵니다.\n","scheduler = StepLR(optimizer, lr_decay_step, gamma=0.5)"],"id":"original-breath","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"resistant-compiler"},"source":["# -- scheduler: ReduceLROnPlateau\n","# 성능이 향상되지 않을 때 learning rate를 줄입니다. patience=10은 10회 동안 성능 향상이 없을 경우입니다.\n","scheduler = ReduceLROnPlateau(optimizer, factor=0.1, patience=10)"],"id":"resistant-compiler","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"distant-crowd"},"source":["# -- scheduler: CosineAnnealingLR\n","# CosineAnnealing은 learning rate를 cosine 그래프처럼 변화시킵니다.\n","scheduler = CosineAnnealingLR(optimizer, T_max=2, eta_min=0.)"],"id":"distant-crowd","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"under-documentary"},"source":["## 4. Metric\n","- Classification 성능을 표현할 때 다양한 평가지표가 있습니다.\n","- Accuracy: 모델이 정확하게 예측한 객체의 비율\n","- True Positive(TP): 실제 True인 정답을 True라고 예측 (정답)\n","- False Positive(FP): 실제 False인 정답을 True라고 예측 (오답)\n","- False Negative(FN): 실제 True인 정답을 False라고 예측 (오답)\n","- True Negative(TN): 실제 False인 정답을 False라고 예측 (정답)\n","- Precision(정밀도): TP / (TP + FP)\n","- Recall(재현율): TP / (TP + FN)"],"id":"under-documentary"},{"cell_type":"code","metadata":{"id":"racial-zealand"},"source":["from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","y_true = [0, 1, 2, 0, 1, 2]\n","y_pred = [0, 2, 1, 0, 0, 1]"],"id":"racial-zealand","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"union-laundry"},"source":["# -- Accuracy\n","accuracy_score(y_true, y_pred)"],"id":"union-laundry","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rotary-working"},"source":["# -- Accuracy\n","# Normalize를 안하면 맞춘 개수가 표시된다\n","accuracy_score(y_true, y_pred, normalize=False)"],"id":"rotary-working","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"employed-torture"},"source":["# -- Precision\n","precision = precision_score(y_true, y_pred, average='macro')\n","precision"],"id":"employed-torture","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"designed-storage"},"source":["# -- Recall\n","recall = recall_score(y_true, y_pred, average='macro')\n","recall"],"id":"designed-storage","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"informed-consolidation"},"source":["# -- f1 score\n","2 * (precision * recall) / (precision + recall)"],"id":"informed-consolidation","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"owned-victory"},"source":["# -- f1 score (sklearn)\n","f1_score(y_true, y_pred, average='macro')"],"id":"owned-victory","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"foreign-guess"},"source":["## 5. Training process"],"id":"foreign-guess"},{"cell_type":"code","metadata":{"id":"several-principal"},"source":["dataset = MaskMultiClassDataset(img_root, label_path, 'train')\n","n_val = int(len(dataset) * val_split)\n","n_train = len(dataset) - n_val\n","train_set, val_set = torch.utils.data.random_split(dataset, [n_train, n_val])\n","val_set.dataset.set_phase(\"test\")  # todo : fix\n","\n","train_loader = torch.utils.data.DataLoader(\n","    train_set,\n","    batch_size=batch_size,\n","    num_workers=num_workers,\n","    drop_last=True,\n",")\n","\n","val_loader = torch.utils.data.DataLoader(\n","    val_set,\n","    batch_size=batch_size,\n","    num_workers=num_workers,\n","    drop_last=True,\n",")"],"id":"several-principal","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bulgarian-moisture"},"source":["### 5.1 Callback - Checkpoint, Early Stopping"],"id":"bulgarian-moisture"},{"cell_type":"code","metadata":{"id":"insured-craft"},"source":["# -- Callback1: Checkpoint - Accuracy가 높아질 때마다 모델을 저장합니다.\n","# 학습 코드에서 이어집니다.\n","\n","# -- Callback2: Early Stopping - 성능이 일정 기간동안 향상이 없을 경우 학습을 종료합니다.\n","patience = 10\n","counter = 0\n","# 학습 코드에서 이어집니다."],"id":"insured-craft","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"turned-citizenship"},"source":["### 5.2 Training Method - Gradient Accumulation\n","- Graident Accumulation은 한 iteration에 파라미터를 업데이트시키는게 아니라, gradient를 여러 iteration 동안 쌓아서 업데이트시킵니다. 한 번에 파라미터를 업데이트시키는 건 noise가 있을 수 있으므로, 여러번 쌓아서 한번에 업데이트 시킴으로써 그러한 문제를 방지하기 위함입니다."],"id":"turned-citizenship"},{"cell_type":"code","metadata":{"id":"bibliographic-recycling"},"source":["# -- Gradient Accumulation\n","accumulation_steps = 2\n","# 학습코드에서 이어집니다."],"id":"bibliographic-recycling","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"premium-poker"},"source":["### 5.3 Training Loop"],"id":"premium-poker"},{"cell_type":"code","metadata":{"id":"24b612c2"},"source":["os.makedirs(os.path.join(os.getcwd(), 'results', name), exist_ok=True)\n","\n","counter = 0\n","best_val_acc = 0\n","best_val_loss = np.inf\n","for epoch in range(num_epochs):\n","    # train loop\n","    model.train()\n","    loss_value = 0\n","    matches = 0\n","    for idx, train_batch in enumerate(train_loader):\n","        inputs, labels = train_batch\n","        inputs = inputs.to(device)\n","        labels = labels.to(device)\n","\n","        outs = model(inputs)\n","        preds = torch.argmax(outs, dim=-1)\n","        loss = criterion(outs, labels)\n","\n","        loss.backward()\n","        \n","        # -- Gradient Accumulation\n","        if (idx+1) % accumulation_steps == 0:\n","            optimizer.step()\n","            optimizer.zero_grad()\n","\n","        loss_value += loss.item()\n","        matches += (preds == labels).sum().item()\n","        if (idx + 1) % train_log_interval == 0:\n","            train_loss = loss_value / train_log_interval\n","            train_acc = matches / batch_size / train_log_interval\n","            current_lr = scheduler.get_last_lr()\n","            print(\n","                f\"Epoch[{epoch}/{num_epochs}]({idx + 1}/{len(train_loader)}) || \"\n","                f\"training loss {train_loss:4.4} || training accuracy {train_acc:4.2%} || lr {current_lr}\"\n","            )\n","\n","            loss_value = 0\n","            matches = 0\n","\n","    scheduler.step()\n","\n","    # val loop\n","    with torch.no_grad():\n","        print(\"Calculating validation results...\")\n","        model.eval()\n","        val_loss_items = []\n","        val_acc_items = []\n","        for val_batch in val_loader:\n","            inputs, labels = val_batch\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","\n","            outs = model(inputs)\n","            preds = torch.argmax(outs, dim=-1)\n","\n","            loss_item = criterion(outs, labels).item()\n","            acc_item = (labels == preds).sum().item()\n","            val_loss_items.append(loss_item)\n","            val_acc_items.append(acc_item)\n","\n","        val_loss = np.sum(val_loss_items) / len(val_loader)\n","        val_acc = np.sum(val_acc_items) / len(val_set)\n","        \n","        # Callback1: validation accuracy가 향상될수록 모델을 저장합니다.\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","        if val_acc > best_val_acc:\n","            print(\"New best model for val accuracy! saving the model..\")\n","            torch.save(model.state_dict(), f\"results/{name}/{epoch:03}_accuracy_{val_acc:4.2%}.ckpt\")\n","            best_val_acc = val_acc\n","            counter = 0\n","        else:\n","            counter += 1\n","        # Callback2: patience 횟수 동안 성능 향상이 없을 경우 학습을 종료시킵니다.\n","        if counter > patience:\n","            print(\"Early Stopping...\")\n","            break\n","        \n","        \n","        print(\n","            f\"[Val] acc : {val_acc:4.2%}, loss: {val_loss:4.2} || \"\n","            f\"best acc : {best_val_acc:4.2%}, best loss: {best_val_loss:4.2}\"\n","        )"],"id":"24b612c2","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"alert-marathon"},"source":["## 6. Reference\n","- [sumni blog post](https://sumniya.tistory.com/26)\n","- [How to create a custom loss function in PyTorch](https://neptune.ai/blog/pytorch-loss-functions)"],"id":"alert-marathon"}]}