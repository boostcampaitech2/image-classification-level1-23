{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "about-heavy",
   "metadata": {},
   "source": [
    "## 0. Libarary 불러오기 및 경로설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cubic-scoop",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize\n",
    "\n",
    "import albumentations\n",
    "from albumentations import *\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import torch.optim as optim\n",
    "from adamp import AdamP\n",
    "\n",
    "import timm\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "import glob\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "built-elevation",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = '/opt/ml/input/data/train'\n",
    "test_dir = '/opt/ml/input/data/eval'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quiet-organizer",
   "metadata": {},
   "source": [
    "## 1. Custom Model 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "domestic-channels",
   "metadata": {},
   "source": [
    "## 2. Custom Dataset 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fce8116-d406-4a4c-a718-48cac5a6753d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, img_paths, labels, masks, genders, ages, transform, device):\n",
    "        \"\"\"Initialize CustomDataset\n",
    "        \n",
    "        Parameters:\n",
    "        img_paths (list of string): list of image paths\n",
    "        labels (list of int): list of labels\n",
    "        \n",
    "        \"\"\"\n",
    "        self.img_paths = img_paths\n",
    "        self.labels = torch.tensor(labels).to(device)\n",
    "        self.masks = torch.tensor(masks).to(device)\n",
    "        self.genders = torch.tensor(genders).to(device)\n",
    "        self.ages = torch.tensor(ages).to(device)\n",
    "        self.device = device\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.img_paths[index])\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image=np.array(image))['image']\n",
    "\n",
    "        return image.to(self.device), (self.labels[index], self.masks[index], self.genders[index], self.ages[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f23752-e401-4619-b093-33e8f29e2176",
   "metadata": {},
   "source": [
    "## 3. Model의 input data, label data 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96e6484b-27b3-4603-91e1-6ea918a1dd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "train_csv = pd.read_csv(os.path.join(train_dir, 'train.csv'))\n",
    "\n",
    "# input data, output data 리스트 만들기(이미지 텐서화는 CustomDataset에서 이뤄짐)\n",
    "train_image_paths = []\n",
    "train_labels = []\n",
    "train_masks = [] # mask:0 / incorrect:1 / notwear:2\n",
    "train_genders = [] # male:0 / female:1\n",
    "train_ages = [] # (,30):0 / [30, 58):1 / [58,):2 \n",
    "\n",
    "dict_mask = {'mask1':0,\n",
    "             'mask2':0,\n",
    "             'mask3':0,\n",
    "             'mask4':0,\n",
    "             'mask5':0,\n",
    "             'incorrect_mask':1,\n",
    "             'normal':2,\n",
    "            }\n",
    "\n",
    "dict_gender = {'male':0,\n",
    "              'female':1}\n",
    "\n",
    "for i in range(train_csv.shape[0]): # number of train image folders is 2700\n",
    "    row = train_csv.loc[i]\n",
    "    seven_paths = glob.glob(train_dir + '/images/' + row['path'] + '/*.*')\n",
    "    \n",
    "    gender = row['gender']\n",
    "    age = row['age']\n",
    "    for i, path in enumerate(seven_paths):\n",
    "        label = 0\n",
    "        mask = path.split('/')[-1].split('.')[0]\n",
    "        mask_label = dict_mask[mask]\n",
    "        gender_label = dict_gender[gender]\n",
    "        age_label = 0\n",
    "        if 30 <= age < 58:\n",
    "            age_label += 1\n",
    "        elif 58 <= age:\n",
    "            age_label += 2\n",
    "            \n",
    "        label = mask_label * 6 + gender_label * 3 + age_label        \n",
    "                    \n",
    "        train_image_paths.append(path)\n",
    "        train_labels.append(label)\n",
    "        train_masks.append(mask_label)\n",
    "        train_genders.append(gender_label)\n",
    "        train_ages.append(age_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d52ae6-a494-485f-b944-9d97c3d3147c",
   "metadata": {},
   "source": [
    "## 4. DataLoader 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f51e120-09e2-4c7d-8964-b0dc6567ad84",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean=(0.548, 0.504, 0.479)\n",
    "std=(0.237, 0.247, 0.246)\n",
    "\n",
    "transform_train = albumentations.Compose([\n",
    "            #Resize(img_size[0], img_size[1], p=1.0),\n",
    "            #Resize(200, 260, p=1.0),\n",
    "            CenterCrop(height = 400, width = 200), # add centercrop 350/350 -> 400/200 -> 300/300\n",
    "            #HorizontalFlip(p=0.5),\n",
    "            #ShiftScaleRotate(p=0.5),\n",
    "            #HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n",
    "            RandomBrightnessContrast(brightness_limit=(-0.1, 0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n",
    "            #GaussNoise(p=0.5),\n",
    "            Normalize(mean=mean, std=std, max_pixel_value=255.0, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.0)\n",
    "\n",
    "transform_val = albumentations.Compose([\n",
    "            #Resize(img_size[0], img_size[1]),\n",
    "            #Resize(200, 260),\n",
    "            CenterCrop(height = 400, width = 200), # add centercrop\n",
    "            Normalize(mean=mean, std=std, max_pixel_value=255.0, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5a3648-7063-4eeb-b91d-57f9786947b0",
   "metadata": {},
   "source": [
    "## 5. Accuracy 계산 함수 정의 및 Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a9613c5-73b5-4b69-81f4-b5d48fef35d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://discuss.pytorch.org/t/is-this-a-correct-implementation-for-focal-loss-in-pytorch/43327/8\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, weight=None,\n",
    "                 gamma=2., reduction='mean'):\n",
    "        nn.Module.__init__(self)\n",
    "        self.weight = weight\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, input_tensor, target_tensor):\n",
    "        log_prob = F.log_softmax(input_tensor, dim=-1)\n",
    "        prob = torch.exp(log_prob)\n",
    "        return F.nll_loss(\n",
    "            ((1 - prob) ** self.gamma) * log_prob,\n",
    "            target_tensor,\n",
    "            weight=self.weight,\n",
    "            reduction=self.reduction\n",
    "        )\n",
    "    \n",
    "class F1Loss(nn.Module):\n",
    "    def __init__(self, classes=18, epsilon=1e-7):\n",
    "        super().__init__()\n",
    "        self.classes = classes\n",
    "        self.epsilon = epsilon\n",
    "    def forward(self, y_pred, y_true):\n",
    "        assert y_pred.ndim == 2\n",
    "        assert y_true.ndim == 1\n",
    "        y_true = F.one_hot(y_true, self.classes).to(torch.float32)\n",
    "        y_pred = F.softmax(y_pred, dim=1)\n",
    "\n",
    "        tp = (y_true * y_pred).sum(dim=0).to(torch.float32)\n",
    "        tn = ((1 - y_true) * (1 - y_pred)).sum(dim=0).to(torch.float32)\n",
    "        fp = ((1 - y_true) * y_pred).sum(dim=0).to(torch.float32)\n",
    "        fn = (y_true * (1 - y_pred)).sum(dim=0).to(torch.float32)\n",
    "\n",
    "        precision = tp / (tp + fp + self.epsilon)\n",
    "        recall = tp / (tp + fn + self.epsilon)\n",
    "\n",
    "        f1 = 2 * (precision * recall) / (precision + recall + self.epsilon)\n",
    "        f1 = f1.clamp(min=self.epsilon, max=1 - self.epsilon)\n",
    "        return 1 - f1.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3abd744-edb6-490d-b414-8cd867c80490",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(train_image_paths, train_labels, train_masks, train_genders, train_ages, transform_train, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "133794a5-9fde-4907-bcc4-0b035432b2db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Stratified_K-Fold :: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3fb1e35a4d74f5da01ce260ffd97771",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='epoch-0'), FloatProgress(value=0.0, max=472.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## Stratified_K-Fold :: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86195d056b724bf18cf23cc85daf2e04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='epoch-0'), FloatProgress(value=0.0, max=472.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## Stratified_K-Fold :: 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2bef7427f184169ab4a6e5f0e0b027a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='epoch-0'), FloatProgress(value=0.0, max=472.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## Stratified_K-Fold :: 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12bfa3ea733b4cfaa40806a6b9006207",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='epoch-0'), FloatProgress(value=0.0, max=472.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## Stratified_K-Fold :: 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "939a8b1c91c84c6bbdec8a1ba695a43a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='epoch-0'), FloatProgress(value=0.0, max=472.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 0, epoch_acc: 0.9411943855932203, epoch_loss: 0.00924037117511034\n",
      "epoch: 0, epoch_val_acc: 0.9625, epoch_loss: 0.005793455988168716\n",
      "epoch_class_acc:\n",
      "class0: 0.974(train_label) / 0.991(val_label)\n",
      "class1: 0.914(train_label) / 0.921(val_label)\n",
      "class2: 0.901(train_label) / 0.980(val_label)\n",
      "class3: 0.979(train_label) / 0.992(val_label)\n",
      "class4: 0.940(train_label) / 0.939(val_label)\n",
      "class5: 0.884(train_label) / 0.932(val_label)\n",
      "class6: 0.948(train_label) / 0.978(val_label)\n",
      "class7: 0.884(train_label) / 0.898(val_label)\n",
      "class8: 0.850(train_label) / 0.972(val_label)\n",
      "class9: 0.968(train_label) / 0.993(val_label)\n",
      "class10: 0.921(train_label) / 0.931(val_label)\n",
      "class11: 0.861(train_label) / 0.946(val_label)\n",
      "class12: 0.968(train_label) / 0.987(val_label)\n",
      "class13: 0.890(train_label) / 0.936(val_label)\n",
      "class14: 0.873(train_label) / 0.955(val_label)\n",
      "class15: 0.976(train_label) / 0.995(val_label)\n",
      "class16: 0.920(train_label) / 0.945(val_label)\n",
      "class17: 0.842(train_label) / 0.864(val_label)\n",
      "\n",
      "epoch_mask_acc:\n",
      "class0: 0.997(train_mask) / 0.998(val_mask)\n",
      "class1: 0.981(train_mask) / 0.992(val_mask)\n",
      "class2: 0.997(train_mask) / 0.999(val_mask)\n",
      "\n",
      "epoch_gender_acc:\n",
      "class0: 0.983(train_gender) / 0.993(val_gender)\n",
      "class1: 0.990(train_gender) / 0.991(val_gender)\n",
      "\n",
      "epoch_age_acc:\n",
      "class0: 0.988(train_age) / 0.999(val_age)\n",
      "class1: 0.942(train_age) / 0.946(val_age)\n",
      "class2: 0.891(train_age) / 0.952(val_age)\n",
      "## Stratified_K-Fold :: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f891088c9bea4ced8201df5c7c8ff6b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='epoch-1'), FloatProgress(value=0.0, max=472.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-27d4f0a86df2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m             \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/adamp/adamp.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 \u001b[0mwd_ratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m                     \u001b[0mperturb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd_ratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_projection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperturb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'delta'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'wd_ratio'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0;31m# Weight decay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/adamp/adamp.py\u001b[0m in \u001b[0;36m_projection\u001b[0;34m(self, p, grad, perturb, delta, wd_ratio, eps)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mcosine_sim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mview_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mcosine_sim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mdelta\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mview_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m                 \u001b[0mp_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mview_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpand_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0mperturb\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mp_n\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mview_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_n\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mperturb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpand_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "from adamp import AdamP\n",
    "import timm\n",
    "from sklearn.model_selection import KFold , StratifiedKFold\n",
    "import copy\n",
    "\n",
    "\n",
    "LEARNING_RATE = 3e-4\n",
    "WEIGHT_DECAY = 1e-5\n",
    "\n",
    "\n",
    "def func_acc(outputs, labels):\n",
    "    cnt_answer = 0\n",
    "    for i in range(len(labels)):\n",
    "        label = labels[i]\n",
    "        _, output = outputs[i].max(dim=0)\n",
    "        if label == output:\n",
    "            cnt_answer += 1\n",
    "        \n",
    "    return cnt_answer / len(labels)\n",
    " \n",
    "    \n",
    "    \n",
    "def func_class_acc(outputs, labels, pre_acc_list):\n",
    "    # return: [[True, False, ], [], [], ...]\n",
    "    answer = [[] for i in range(18)]    \n",
    "    \n",
    "    is_answer = [[] for i in range(18)]\n",
    "    for i in range(len(labels)):\n",
    "        label = labels[i]\n",
    "        _, output = outputs[i].max(dim=0)\n",
    "        if label == output:\n",
    "            is_answer[label].append(True)\n",
    "        else:\n",
    "            is_answer[label].append(False)\n",
    "            \n",
    "    for i in range(18):\n",
    "        answer[i] = pre_acc_list[i] + is_answer[i]\n",
    "                           \n",
    "    return answer\n",
    "\n",
    "\n",
    "def func_class_acc_mask(outputs, labels, pre_acc_list):\n",
    "    # return: [[True, False, ], [], [], ...]\n",
    "    answer = [[] for i in range(3)]    \n",
    "    \n",
    "    is_answer = [[] for i in range(3)]\n",
    "    for i in range(len(labels)):\n",
    "        label = labels[i]\n",
    "        _, output = outputs[i].max(dim=0)\n",
    "        if label == output:\n",
    "            is_answer[label].append(True)\n",
    "        else:\n",
    "            is_answer[label].append(False)\n",
    "            \n",
    "    for i in range(3):\n",
    "        answer[i] = pre_acc_list[i] + is_answer[i]\n",
    "                           \n",
    "    return answer\n",
    "\n",
    "\n",
    "def func_class_acc_gender(outputs, labels, pre_acc_list):\n",
    "    # return: [[True, False, ], [], [], ...]\n",
    "    answer = [[] for i in range(2)]    \n",
    "    \n",
    "    is_answer = [[] for i in range(2)]\n",
    "    for i in range(len(labels)):\n",
    "        label = labels[i]\n",
    "        _, output = outputs[i].max(dim=0)\n",
    "        if label == output:\n",
    "            is_answer[label].append(True)\n",
    "        else:\n",
    "            is_answer[label].append(False)\n",
    "            \n",
    "    for i in range(2):\n",
    "        answer[i] = pre_acc_list[i] + is_answer[i]\n",
    "                           \n",
    "    return answer\n",
    "\n",
    "\n",
    "def func_class_acc_age(outputs, labels, pre_acc_list):\n",
    "    # return: [[True, False, ], [], [], ...]\n",
    "    answer = [[] for i in range(3)]    \n",
    "    \n",
    "    is_answer = [[] for i in range(3)]\n",
    "    for i in range(len(labels)):\n",
    "        label = labels[i]\n",
    "        _, output = outputs[i].max(dim=0)\n",
    "        if label == output:\n",
    "            is_answer[label].append(True)\n",
    "        else:\n",
    "            is_answer[label].append(False)\n",
    "            \n",
    "    for i in range(3):\n",
    "        answer[i] = pre_acc_list[i] + is_answer[i]\n",
    "                           \n",
    "    return answer\n",
    "\n",
    "\n",
    "def cal_class_acc(epoch_class_acc):\n",
    "    output = [0] * 18\n",
    "    for i in range(18):\n",
    "        total_cnt = len(epoch_class_acc[i])\n",
    "        answer_cnt = 0\n",
    "        for answer in epoch_class_acc[i]:\n",
    "            if answer:\n",
    "                answer_cnt += 1\n",
    "        output[i] = answer_cnt / total_cnt\n",
    "    return output\n",
    "\n",
    "\n",
    "def cal_class_acc_mask(epoch_class_acc):\n",
    "    output = [0] * 3\n",
    "    for i in range(3):\n",
    "        total_cnt = len(epoch_class_acc[i])\n",
    "        answer_cnt = 0\n",
    "        for answer in epoch_class_acc[i]:\n",
    "            if answer:\n",
    "                answer_cnt += 1\n",
    "        output[i] = answer_cnt / total_cnt\n",
    "    return output\n",
    "\n",
    "\n",
    "def cal_class_acc_gender(epoch_class_acc):\n",
    "    output = [0] * 2\n",
    "    for i in range(2):\n",
    "        total_cnt = len(epoch_class_acc[i])\n",
    "        answer_cnt = 0\n",
    "        for answer in epoch_class_acc[i]:\n",
    "            if answer:\n",
    "                answer_cnt += 1\n",
    "        output[i] = answer_cnt / total_cnt\n",
    "    return output\n",
    "\n",
    "\n",
    "def cal_class_acc_age(epoch_class_acc):\n",
    "    output = [0] * 3\n",
    "    for i in range(3):\n",
    "        total_cnt = len(epoch_class_acc[i])\n",
    "        answer_cnt = 0\n",
    "        for answer in epoch_class_acc[i]:\n",
    "            if answer:\n",
    "                answer_cnt += 1\n",
    "        output[i] = answer_cnt / total_cnt\n",
    "    return output\n",
    "\n",
    "\n",
    "#outputs_label = func_labels(outputs_mask, outputs_gender, outputs_age)\n",
    "def func_labels(outputs_mask, outputs_gender, outputs_age, device):\n",
    "    # outputs_label = [class1, class2, class3, ...]\n",
    "    outputs_label = torch.Tensor([])\n",
    "    len_outputs = len(outputs_mask)\n",
    "    for i in range(len_outputs):\n",
    "        mask_class = outputs_mask[i] # [0.6, 0.2, 0.1, 0.1]\n",
    "        _, mask_class = mask_class.max(dim=0)\n",
    "        \n",
    "        gender_class = outputs_gender[i] # [0.6, 0.2, 0.1, 0.1]\n",
    "        _, gender_class = gender_class.max(dim=0)\n",
    "        \n",
    "        age_class = outputs_age[i] # [0.6, 0.2, 0.1, 0.1]\n",
    "        _, age_class = age_class.max(dim=0)\n",
    "        \n",
    "        label = mask_class * 6 + gender_class * 3 + age_class\n",
    "        \n",
    "        #label: int -> [[1, 0, 0, 0]]\n",
    "        one_hot = torch.zeros((1,18))\n",
    "        one_hot[0][label] = 1\n",
    "        label = one_hot\n",
    "        outputs_label = torch.cat([outputs_label, label])\n",
    "    return outputs_label.to(device)\n",
    "        \n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.model = timm.create_model('tf_efficientnet_b4', pretrained=True)\n",
    "        self.model.classifier = nn.Linear(1792, 1024)\n",
    "        self.fc1 = nn.Linear(1024, 3)\n",
    "        self.fc2 = nn.Linear(1024, 2)\n",
    "        self.fc3 = nn.Linear(1024, 3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        fc_output = self.model(x)\n",
    "        mask = self.fc1(fc_output)\n",
    "        gender = self.fc2(fc_output)\n",
    "        age = self.fc3(fc_output)\n",
    "        \n",
    "        return mask, gender, age      \n",
    "\n",
    "    \n",
    "model = MyModel().to(device)\n",
    "    \n",
    "criterion = FocalLoss(gamma = 5)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# criterion = F1Loss()\n",
    "\n",
    "optimizer = AdamP(model.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.999), weight_decay= WEIGHT_DECAY)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[500,1000,1500], gamma=0.5)\n",
    "\n",
    "min_loss = float('inf')\n",
    "\n",
    "for epoch in range(10):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_val_loss = 0\n",
    "    epoch_val_acc = 0\n",
    "\n",
    "    epoch_class_acc = [[] for i in range(18)]\n",
    "    epoch_class_val_acc = [[] for i in range(18)]\n",
    "    \n",
    "    epoch_mask_acc = [[] for i in range(18)]\n",
    "    epoch_mask_val_acc = [[] for i in range(18)]\n",
    "    \n",
    "    epoch_gender_acc = [[] for i in range(18)]\n",
    "    epoch_gender_val_acc = [[] for i in range(18)]\n",
    "    \n",
    "    epoch_age_acc = [[] for i in range(18)]\n",
    "    epoch_age_val_acc = [[] for i in range(18)]\n",
    "    \n",
    "    \n",
    "    stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=None)\n",
    "    k_idx = 1\n",
    "    for train_index, validate_index in stratified_kfold.split(np.zeros(len(train_ages)), train_ages):  \n",
    "        print(f'## Stratified_K-Fold :: {k_idx}')\n",
    "        k_idx += 1\n",
    "        train_dataset = torch.utils.data.dataset.Subset(dataset, train_index)\n",
    "        valid_dataset = torch.utils.data.dataset.Subset(dataset, validate_index)\n",
    "        valid_dataset = copy.deepcopy(valid_dataset)\n",
    "        valid_dataset.dataset.transform = transform_val\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset,\n",
    "                    batch_size=32,\n",
    "                    shuffle=True,\n",
    "                    num_workers=0,\n",
    "                    drop_last=True \n",
    "                   )\n",
    "        \n",
    "        val_loader = DataLoader(valid_dataset,\n",
    "                    batch_size = 32,\n",
    "                    shuffle=True,\n",
    "                    num_workers=0\n",
    "                   )\n",
    "\n",
    "        \n",
    "        for i, data in tqdm(enumerate(train_loader), desc=f\"epoch-{epoch}\", total=len(train_loader)):\n",
    "            inputs, (labels, masks, genders, ages) = data\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs_mask, outputs_gender, outputs_age = model(inputs)\n",
    "            outputs_label = func_labels(outputs_mask, outputs_gender, outputs_age, device)\n",
    "\n",
    "            loss_masks = criterion(outputs_mask, masks)\n",
    "            loss_genders = criterion(outputs_gender, genders)\n",
    "            loss_ages = criterion(outputs_age, ages)\n",
    "\n",
    "            loss = loss_masks + loss_genders + loss_ages\n",
    "\n",
    "            epoch_loss += loss\n",
    "\n",
    "            acc = func_acc(outputs_label, labels)\n",
    "            epoch_acc += acc     \n",
    "\n",
    "            epoch_class_acc = func_class_acc(outputs_label, labels, epoch_class_acc)\n",
    "\n",
    "            epoch_mask_acc = func_class_acc_mask(outputs_mask, masks, epoch_mask_acc)\n",
    "            epoch_gender_acc = func_class_acc_gender(outputs_gender, genders, epoch_gender_acc)\n",
    "            epoch_age_acc = func_class_acc_age(outputs_age, ages, epoch_age_acc)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(val_loader):\n",
    "                val_inputs, (val_labels, val_masks, val_genders, val_ages) = data\n",
    "                val_outputs_mask, val_outputs_gender, val_outputs_age = model(val_inputs)\n",
    "                val_outputs_label = func_labels(val_outputs_mask, val_outputs_gender, val_outputs_age, device)\n",
    "\n",
    "                val_loss_mask = criterion(val_outputs_mask, val_masks)\n",
    "                val_loss_gender = criterion(val_outputs_gender, val_genders)\n",
    "                val_loss_age = criterion(val_outputs_age, val_ages)\n",
    "                val_loss = val_loss_mask + val_loss_gender + val_loss_age\n",
    "\n",
    "                epoch_val_loss += val_loss\n",
    "\n",
    "                val_acc = func_acc(val_outputs_label, val_labels)\n",
    "                epoch_val_acc += val_acc\n",
    "\n",
    "                epoch_class_val_acc = func_class_acc(val_outputs_label, val_labels, epoch_class_val_acc)\n",
    "                epoch_mask_val_acc = func_class_acc_mask(val_outputs_mask, val_masks, epoch_mask_val_acc)\n",
    "                epoch_gender_val_acc = func_class_acc_gender(val_outputs_gender, val_genders, epoch_gender_val_acc)\n",
    "                epoch_age_val_acc = func_class_acc_age(val_outputs_age, val_ages, epoch_age_val_acc)\n",
    "\n",
    "\n",
    "\n",
    "    epoch_loss /= len(train_loader) * 5\n",
    "    epoch_acc /= len(train_loader) * 5\n",
    "    epoch_class_acc = cal_class_acc(epoch_class_acc)\n",
    "    \n",
    "    epoch_mask_acc = cal_class_acc_mask(epoch_mask_acc)\n",
    "    epoch_gender_acc = cal_class_acc_gender(epoch_gender_acc)\n",
    "    epoch_age_acc = cal_class_acc_age(epoch_age_acc)\n",
    "\n",
    "    epoch_val_loss /= len(val_loader) * 5\n",
    "    epoch_val_acc /= len(val_loader) * 5\n",
    "    epoch_class_val_acc = cal_class_acc(epoch_class_val_acc)\n",
    "    epoch_mask_val_acc = cal_class_acc_mask(epoch_mask_val_acc)\n",
    "    epoch_gender_val_acc = cal_class_acc_gender(epoch_gender_val_acc)\n",
    "    epoch_age_val_acc = cal_class_acc_age(epoch_age_val_acc)\n",
    "    \n",
    "    if min_loss > epoch_loss:\n",
    "        save_path = f'./save_model/epoch_{epoch+1}_loss_{epoch_loss}.pth'\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        min_loss = epoch_loss\n",
    "    \n",
    "    print(f'epoch: {epoch}, epoch_acc: {epoch_acc}, epoch_loss: {epoch_loss}')\n",
    "    \n",
    "    print(f'epoch: {epoch}, epoch_val_acc: {epoch_val_acc}, epoch_val_loss: {epoch_val_loss}')    \n",
    "    print(f'epoch_class_acc:')\n",
    "    for class_id in range(18):\n",
    "        print(f'class{class_id}: {epoch_class_acc[class_id]:.3f}(train_label) / {epoch_class_val_acc[class_id]:.3f}(val_label)')\n",
    "    \n",
    "    print(f'\\nepoch_mask_acc:')\n",
    "    for class_id in range(3):\n",
    "        print(f'class{class_id}: {epoch_mask_acc[class_id]:.3f}(train_mask) / {epoch_mask_val_acc[class_id]:.3f}(val_mask)')\n",
    "    \n",
    "    print(f'\\nepoch_gender_acc:')\n",
    "    for class_id in range(2):\n",
    "        print(f'class{class_id}: {epoch_gender_acc[class_id]:.3f}(train_gender) / {epoch_gender_val_acc[class_id]:.3f}(val_gender)')                \n",
    "    \n",
    "    print(f'\\nepoch_age_acc:')\n",
    "    for class_id in range(3):\n",
    "        print(f'class{class_id}: {epoch_age_acc[class_id]:.3f}(train_age) / {epoch_age_val_acc[class_id]:.3f}(val_age)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8077ef-6cc9-4b07-a702-8634fb6cca0e",
   "metadata": {},
   "source": [
    "0: 2745<br/>\n",
    "1: 2050<br/>\n",
    "2: 415<br/>\n",
    "3: 3660<br/>\n",
    "4: 4085<br/>\n",
    "5: 545<br/>\n",
    "6: 549<br/>\n",
    "7: 410<br/>\n",
    "8: 83<br/>\n",
    "9: 732<br/>\n",
    "10: 817<br/>\n",
    "11: 109<br/>\n",
    "12: 549<br/>\n",
    "13: 410<br/>\n",
    "14: 83<br/>\n",
    "15: 732<br/>\n",
    "16: 817<br/>\n",
    "17: 109<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175b761c-2920-42ec-a77b-2677c59b1927",
   "metadata": {},
   "source": [
    "총이미지: 2700 * 3 배치사이즈: 32 한 epoch당 253번의 iteration 발생<br/>\n",
    "총이미지: 2700 * 3 배치사이즈: 64 한 epoch당 126번의 iteration 발생<br/>\n",
    "총이미지: 2700 * 3 배치사이즈: 128 한 epoch당 63번의 iteration 발생<br/>\n",
    "총이미지: 2700 * 3 배치사이즈: 256 한 epoch당 31번의 iteration 발생<br/>\n",
    "\n",
    "총이미지: 18828 배치사이즈:256 한 epoch당 74번의 iteration 발생"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-feelings",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 6. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5b6e6931-f5a8-4639-bae7-20f66517f4a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test inference is done!\n"
     ]
    }
   ],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, img_paths, transform, device):\n",
    "        self.img_paths = img_paths\n",
    "        self.device = device\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.img_paths[index])\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image=np.array(image))['image']\n",
    "\n",
    "        return image.to(self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "\n",
    "    \n",
    "# meta 데이터와 이미지 경로를 불러옵니다.\n",
    "submission = pd.read_csv(os.path.join(test_dir, 'info.csv'))\n",
    "image_dir = os.path.join(test_dir, 'images')\n",
    "\n",
    "# Test Dataset 클래스 객체를 생성하고 DataLoader를 만듭니다.\n",
    "image_paths = [os.path.join(image_dir, img_id) for img_id in submission.ImageID]\n",
    "transform = albumentations.Compose([\n",
    "            #Resize(img_size[0], img_size[1]),\n",
    "            #Resize(200, 260),\n",
    "            CenterCrop(height = 400, width = 200), # add centercrop\n",
    "            Normalize(mean=mean, std=std, max_pixel_value=255.0, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.0)\n",
    "dataset = TestDataset(image_paths, transform, device)\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# 모델을 정의합니다. (학습한 모델이 있다면 torch.load로 모델을 불러주세요!)\n",
    "device = torch.device('cuda')\n",
    "\n",
    "model = MyModel().to(device)\n",
    "\n",
    "path = './save_model/epoch_1_loss_0.00924037117511034.pth'\n",
    "model.load_state_dict(torch.load(path))\n",
    "model.eval()\n",
    "\n",
    "# 모델이 테스트 데이터셋을 예측하고 결과를 저장합니다.\n",
    "all_predictions = []\n",
    "for images in loader:\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        '''\n",
    "        val_outputs_mask, val_outputs_gender, val_outputs_age = model(val_inputs)\n",
    "        val_outputs_label = func_labels(val_outputs_mask, val_outputs_gender, val_outputs_age, device)\n",
    "        '''\n",
    "#         pred = model(images)\n",
    "        pred_outputs_mask, pred_outputs_gender, pred_outputs_age = model(images)\n",
    "        pred = func_labels(pred_outputs_mask, pred_outputs_gender, pred_outputs_age, device)\n",
    "    \n",
    "        pred = pred.argmax(dim=-1)\n",
    "        all_predictions.extend(pred.cpu().numpy())\n",
    "submission['ans'] = all_predictions\n",
    "\n",
    "# 제출할 파일을 저장합니다.\n",
    "submission.to_csv(os.path.join(test_dir, 'submission.csv'), index=False)\n",
    "print('test inference is done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa2f17a-e1a7-4c34-8181-3276796c8d02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
