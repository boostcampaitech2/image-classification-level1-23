{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3.8.3 64-bit ('aitech': pyenv)"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"colab":{"name":"5_Pretrained Model.ipynb","provenance":[],"collapsed_sections":[]},"interpreter":{"hash":"0964e3f95126ddd6db105c73e10d4f1245c8e331a9ca081f1675c5dba7db3ce0"}},"cells":[{"cell_type":"markdown","source":["## Lesson 6 - Pretrained Model\n"," - 이번 실습 자료에서는 강의시간에 다루었던 torchvision 을 사용하여 pretrained 모델을 사용하는 방법에 대해 실습하겠습니다.\n"," - torchvision 의 pretrained model 리스트는 다음과 같습니다\n"," \n"," [List of torchvision models](https://github.com/pytorch/vision/blob/master/torchvision/models/__init__.py#L1-L14)\n","```\n","from .alexnet import *\n","from .resnet import *\n","from .vgg import *\n","from .squeezenet import *\n","from .inception import *\n","from .densenet import *\n","from .googlenet import *\n","from .mobilenet import *\n","from .mnasnet import *\n","from .shufflenetv2 import *\n","from . import segmentation\n","from . import detection\n","from . import video\n","from . import quantization\n","```"],"metadata":{"id":"-MS3bWUNUSgz"}},{"cell_type":"code","execution_count":null,"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F"],"outputs":[],"metadata":{"id":"QfXG-tzwUSgv"}},{"cell_type":"markdown","source":["#### 가장 기본이라고 할 수 있는 Alextnet 모델 아키텍쳐를 사용해보겠습니다."],"metadata":{"id":"D2BlaYvVUSg0"}},{"cell_type":"code","execution_count":null,"source":["from torchvision.models import alexnet\n","model = alexnet()\n","model"],"outputs":[],"metadata":{"scrolled":true,"id":"8PT7gW-wUSg0"}},{"cell_type":"markdown","source":["#### Alexnet 의 pretrained 버전 또한 쉽게 불러올 수 있습니다."],"metadata":{"id":"kzOWtmBxUSg2"}},{"cell_type":"code","execution_count":null,"source":["model = alexnet(pretrained=True)\n","model"],"outputs":[],"metadata":{"scrolled":true,"id":"36Rqc5-OUSg2"}},{"cell_type":"markdown","source":["#### torchvision 에서 해당 모델을 어떤 식으로 구현하였는지 직접 확인해보면 매우 도움이 많으 됩니다.\n","Example:\n","[source code](https://github.com/pytorch/vision/blob/master/torchvision/models/alexnet.py#L15-L50)\n","```\n","class AlexNet(nn.Module):\n","\n","    def __init__(self, num_classes=1000):\n","        super(AlexNet, self).__init__()\n","        self.features = nn.Sequential(\n","            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=3, stride=2),\n","            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=3, stride=2),\n","            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=3, stride=2),\n","        )\n","        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n","        self.classifier = nn.Sequential(\n","            nn.Dropout(),\n","            nn.Linear(256 * 6 * 6, 4096),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(),\n","            nn.Linear(4096, 4096),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(4096, num_classes),\n","        )\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = self.avgpool(x)\n","        x = torch.flatten(x, 1)\n","        x = self.classifier(x)\n","        return x\n","```"],"metadata":{"id":"3799RyCBUSg3"}},{"cell_type":"markdown","source":["#### 다른 모델들( e.g. vgg19, resnet18) 도 같은 방법으로 손 쉽게 사용할 수 있습니다."],"metadata":{"id":"GCagXJS2USg4"}},{"cell_type":"code","execution_count":null,"source":["from torchvision.models import vgg19_bn\n","model = vgg19_bn(pretrained=True)\n","model"],"outputs":[],"metadata":{"scrolled":true,"id":"tsZbkfObUSg5"}},{"cell_type":"markdown","source":["#### Pretrained 모델을 내 태스크에 맞게 어떻게 사용할 수 있나요?\n"," - Trochvision 모델들은 보통 feature-extraction 파트, task-specific 파트로 크게 두 가지로 구성되어 있습니다.\n"," - Task specific 파트는 모델의 태스크(이미지 분류, 객체 인식 등) 에 따라 모두 다릅니다.\n"," - 심지어 같은 이미지 분류 안에서도, 어떤 데이터셋으로 pretrain 하였느냐에 따라 다를 수 있습니다.\n"," - 따라서, 우리도 우리 테스크에 맞게 task specific 파트는 새로 정의하여 사용하여야 합니다."],"metadata":{"id":"foiF_enTUSg5"}},{"cell_type":"markdown","source":[" - 주로 이미지넷 데이터셋을 사용하여 pretrain 을 하기에 output_dim=1000 인 경우가 많습니다.\n"," - 따라서 우리 태스크의 클래스 갯수(18)에 맞게 재정의하여 사용할 수 있습니다."],"metadata":{"id":"nYv1DZ69USg6"}},{"cell_type":"code","execution_count":null,"source":["num_classes = 18\n","model = vgg19_bn(pretrained=True)\n","model.classifier = nn.Sequential(\n","    nn.Linear(512 * 7 * 7, 4096),\n","    nn.ReLU(True),\n","    nn.Dropout(),\n","    nn.Linear(4096, 4096),\n","    nn.ReLU(True),\n","    nn.Dropout(),\n","    nn.Linear(4096, num_classes),\n",")\n","\n","model"],"outputs":[],"metadata":{"scrolled":true,"id":"xQIxW_QRUSg6"}},{"cell_type":"markdown","source":["#### Weight Freeze\n"," - Weight freeze 란 해당 모듈의 graident 는 역전파 하지 않아 학습을 하지 않는다는 의미입니다.\n"," - 예를 들어, 우리가 하려는 태스크가 pretrain 한 태스크와 매우 유사하다면, feature 파트는 freeze 하여 학습하지 않고 새로 정의한 task specific 파트만 학습하는 것이 좋은 방법일 수 있습니다.\n"," - weight freeze 는 `requires_grad` 를 사용하여 쉽게 구현할 수 있습니다. "],"metadata":{"id":"bkuxurF2USg7"}},{"cell_type":"code","execution_count":null,"source":["# feature 파트만 freeze\n","model.features.requires_grad_(False)\n","for param, weight in model.named_parameters():\n","    print(f\"파라미터 {param:20} 가 gradient 를 tracking 하나요? -> {weight.requires_grad}\")"],"outputs":[],"metadata":{"scrolled":true,"id":"Qr2vjssZUSg7"}},{"cell_type":"markdown","source":["#### Weight initialization \n"," - weight 초기화는 종종 모델의 성능에 critical 한 영향을 줍니다.\n"," - 하지만 만약 pretrained 모델을 사용한다면 pretrained 부분은 초기화를 하지 말고, 재정의한 task specific 파트만 초기화하여야 합니다."],"metadata":{"id":"LM4aPLqeUSg8"}},{"cell_type":"code","execution_count":null,"source":["import torch.nn.init as init\n","\n","def initialize_weights(model):\n","    \"\"\"\n","    Xavier uniform 분포로 모든 weight 를 초기화합니다.\n","    더 많은 weight 초기화 방법은 다음 문서에서 참고해주세요. https://pytorch.org/docs/stable/nn.init.html\n","    \"\"\"\n","    for m in model.modules():\n","        if isinstance(m, nn.Conv2d):\n","            init.xavier_uniform_(m.weight.data)\n","            if m.bias is not None:\n","                m.bias.data.zero_()\n","        elif isinstance(m, nn.BatchNorm2d):\n","            m.weight.data.fill_(1)\n","            m.bias.data.zero_()\n","        elif isinstance(m, nn.Linear):\n","            m.weight.data.normal_(0, 0.01)\n","            m.bias.data.zero_()"],"outputs":[],"metadata":{"id":"I_5s0otKUSg8"}},{"cell_type":"markdown","source":["#### pretrained 모델을 가져와 가장 앞단 layer 의 weight 분포를 봐봅시다"],"metadata":{"id":"mRDhT8vnUSg9"}},{"cell_type":"code","execution_count":null,"source":["import matplotlib.pyplot as plt\n","\n","model = vgg19_bn(pretrained=True)\n","\n","# Weight Initialization 이전 모델 feature 파트의 첫번째 weight 분포\n","plt.hist(model.features[0].weight.detach().numpy().reshape(-1))  \n","plt.show()"],"outputs":[],"metadata":{"id":"GNghuOsYUSg9"}},{"cell_type":"markdown","source":["#### weight 초기화 후 분포를 봐 봅시다\n"," - `xavier_uniform` 으로 초기화하여 웨이트들이 uniform 한 분포를 가지게 되었습니다."],"metadata":{"id":"yeAhmcpbUSg-"}},{"cell_type":"code","execution_count":null,"source":["model = vgg19_bn(pretrained=True)\n","\n","# 모든 weight 를 initialize\n","initialize_weights(model.features)\n","\n","# Weight Initialization 이후 모델 feature 파트의 첫번째 weight 분포\n","# (xavier) uniform 한 분포로 바뀐 것을 확인할 수 있습니다.\n","plt.hist(model.features[0].weight.detach().numpy().reshape(-1))\n","plt.show()"],"outputs":[],"metadata":{"id":"ISYQG_TDUSg-"}},{"cell_type":"markdown","source":["#### task specific 한 부분만 초기화하엿습니다\n"," - feature extraction 파트는 초기화가 되지 않은 것은 확인할 수 있습니다."],"metadata":{"id":"fbqr7onnUSg_"}},{"cell_type":"code","execution_count":null,"source":["model = vgg19_bn(pretrained=True)\n","\n","# Classifier 부분만 initialize\n","initialize_weights(model.classifier)\n","\n","# Weight Initialization 이후 모델 feature 파트의 첫번째 weight 분포\n","# classifier 부분만 xavier uniform 으로 초기화해서 feature 파트는 uniform 한 분포를 가지지 않는 것을 확인할 수 있습니다.\n","plt.hist(model.features[0].weight.detach().numpy().reshape(-1)) \n","plt.show()"],"outputs":[],"metadata":{"id":"kvlDANr6USg_"}},{"cell_type":"markdown","source":["## Appendix (optional)"],"metadata":{"id":"o5rYLwDKUSg_"}},{"cell_type":"markdown","source":["### SOTA (State Of The Art)  모델을 리서치 하는 방법\n","- timm\n","- paper with code"],"metadata":{"id":"DYtGOREKUShA"}},{"cell_type":"markdown","source":["## timm (pyTorch IMage Models)\n","\n","PyTorch Image Models (timm) is a collection of image models, layers, utilities, optimizers, schedulers, data-loaders / augmentations, and reference training / validation scripts that aim to pull together a wide variety of SOTA models with ability to reproduce ImageNet training results.\n","\n","#### References\n","https://github.com/rwightman/pytorch-image-models#introduction\n","\n","https://fastai.github.io/timmdocs/\n","\n","https://rwightman.github.io/pytorch-image-models/"],"metadata":{"id":"2IlmlHvDUShA"}},{"cell_type":"code","execution_count":null,"source":["!pip install timm"],"outputs":[],"metadata":{"id":"qLTgGr7EUShA"}},{"cell_type":"markdown","source":["#### Timm 을 사용하여 pretrained 모델 불러오기"],"metadata":{"id":"yF6B71PxUShA"}},{"cell_type":"code","execution_count":null,"source":["import timm\n","\n","m = timm.create_model('mobilenetv3_large_100', pretrained=True)\n","m.eval()"],"outputs":[],"metadata":{"id":"R2dV1z-1UShB"}},{"cell_type":"markdown","source":["#### Timm 에서 사용가능한 pretrained 모델 목록"],"metadata":{"id":"NxpGBf1HUShB"}},{"cell_type":"code","execution_count":1,"source":["import timm\n","from pprint import pprint\n","model_names = timm.list_models(pretrained=True)\n","pprint(model_names)"],"outputs":[{"output_type":"stream","name":"stdout","text":["['adv_inception_v3',\n"," 'cait_m36_384',\n"," 'cait_m48_448',\n"," 'cait_s24_224',\n"," 'cait_s24_384',\n"," 'cait_s36_384',\n"," 'cait_xs24_384',\n"," 'cait_xxs24_224',\n"," 'cait_xxs24_384',\n"," 'cait_xxs36_224',\n"," 'cait_xxs36_384',\n"," 'coat_lite_mini',\n"," 'coat_lite_small',\n"," 'coat_lite_tiny',\n"," 'coat_mini',\n"," 'coat_tiny',\n"," 'convit_base',\n"," 'convit_small',\n"," 'convit_tiny',\n"," 'cspdarknet53',\n"," 'cspresnet50',\n"," 'cspresnext50',\n"," 'deit_base_distilled_patch16_224',\n"," 'deit_base_distilled_patch16_384',\n"," 'deit_base_patch16_224',\n"," 'deit_base_patch16_384',\n"," 'deit_small_distilled_patch16_224',\n"," 'deit_small_patch16_224',\n"," 'deit_tiny_distilled_patch16_224',\n"," 'deit_tiny_patch16_224',\n"," 'densenet121',\n"," 'densenet161',\n"," 'densenet169',\n"," 'densenet201',\n"," 'densenetblur121d',\n"," 'dla34',\n"," 'dla46_c',\n"," 'dla46x_c',\n"," 'dla60',\n"," 'dla60_res2net',\n"," 'dla60_res2next',\n"," 'dla60x',\n"," 'dla60x_c',\n"," 'dla102',\n"," 'dla102x',\n"," 'dla102x2',\n"," 'dla169',\n"," 'dm_nfnet_f0',\n"," 'dm_nfnet_f1',\n"," 'dm_nfnet_f2',\n"," 'dm_nfnet_f3',\n"," 'dm_nfnet_f4',\n"," 'dm_nfnet_f5',\n"," 'dm_nfnet_f6',\n"," 'dpn68',\n"," 'dpn68b',\n"," 'dpn92',\n"," 'dpn98',\n"," 'dpn107',\n"," 'dpn131',\n"," 'eca_nfnet_l0',\n"," 'eca_nfnet_l1',\n"," 'eca_nfnet_l2',\n"," 'ecaresnet26t',\n"," 'ecaresnet50d',\n"," 'ecaresnet50d_pruned',\n"," 'ecaresnet50t',\n"," 'ecaresnet101d',\n"," 'ecaresnet101d_pruned',\n"," 'ecaresnet269d',\n"," 'ecaresnetlight',\n"," 'efficientnet_b0',\n"," 'efficientnet_b1',\n"," 'efficientnet_b1_pruned',\n"," 'efficientnet_b2',\n"," 'efficientnet_b2_pruned',\n"," 'efficientnet_b3',\n"," 'efficientnet_b3_pruned',\n"," 'efficientnet_b4',\n"," 'efficientnet_el',\n"," 'efficientnet_el_pruned',\n"," 'efficientnet_em',\n"," 'efficientnet_es',\n"," 'efficientnet_es_pruned',\n"," 'efficientnet_lite0',\n"," 'efficientnetv2_rw_m',\n"," 'efficientnetv2_rw_s',\n"," 'ens_adv_inception_resnet_v2',\n"," 'ese_vovnet19b_dw',\n"," 'ese_vovnet39b',\n"," 'fbnetc_100',\n"," 'gernet_l',\n"," 'gernet_m',\n"," 'gernet_s',\n"," 'ghostnet_100',\n"," 'gluon_inception_v3',\n"," 'gluon_resnet18_v1b',\n"," 'gluon_resnet34_v1b',\n"," 'gluon_resnet50_v1b',\n"," 'gluon_resnet50_v1c',\n"," 'gluon_resnet50_v1d',\n"," 'gluon_resnet50_v1s',\n"," 'gluon_resnet101_v1b',\n"," 'gluon_resnet101_v1c',\n"," 'gluon_resnet101_v1d',\n"," 'gluon_resnet101_v1s',\n"," 'gluon_resnet152_v1b',\n"," 'gluon_resnet152_v1c',\n"," 'gluon_resnet152_v1d',\n"," 'gluon_resnet152_v1s',\n"," 'gluon_resnext50_32x4d',\n"," 'gluon_resnext101_32x4d',\n"," 'gluon_resnext101_64x4d',\n"," 'gluon_senet154',\n"," 'gluon_seresnext50_32x4d',\n"," 'gluon_seresnext101_32x4d',\n"," 'gluon_seresnext101_64x4d',\n"," 'gluon_xception65',\n"," 'gmixer_24_224',\n"," 'gmlp_s16_224',\n"," 'hardcorenas_a',\n"," 'hardcorenas_b',\n"," 'hardcorenas_c',\n"," 'hardcorenas_d',\n"," 'hardcorenas_e',\n"," 'hardcorenas_f',\n"," 'hrnet_w18',\n"," 'hrnet_w18_small',\n"," 'hrnet_w18_small_v2',\n"," 'hrnet_w30',\n"," 'hrnet_w32',\n"," 'hrnet_w40',\n"," 'hrnet_w44',\n"," 'hrnet_w48',\n"," 'hrnet_w64',\n"," 'ig_resnext101_32x8d',\n"," 'ig_resnext101_32x16d',\n"," 'ig_resnext101_32x32d',\n"," 'ig_resnext101_32x48d',\n"," 'inception_resnet_v2',\n"," 'inception_v3',\n"," 'inception_v4',\n"," 'legacy_senet154',\n"," 'legacy_seresnet18',\n"," 'legacy_seresnet34',\n"," 'legacy_seresnet50',\n"," 'legacy_seresnet101',\n"," 'legacy_seresnet152',\n"," 'legacy_seresnext26_32x4d',\n"," 'legacy_seresnext50_32x4d',\n"," 'legacy_seresnext101_32x4d',\n"," 'levit_128',\n"," 'levit_128s',\n"," 'levit_192',\n"," 'levit_256',\n"," 'levit_384',\n"," 'mixer_b16_224',\n"," 'mixer_b16_224_in21k',\n"," 'mixer_b16_224_miil',\n"," 'mixer_b16_224_miil_in21k',\n"," 'mixer_l16_224',\n"," 'mixer_l16_224_in21k',\n"," 'mixnet_l',\n"," 'mixnet_m',\n"," 'mixnet_s',\n"," 'mixnet_xl',\n"," 'mnasnet_100',\n"," 'mobilenetv2_100',\n"," 'mobilenetv2_110d',\n"," 'mobilenetv2_120d',\n"," 'mobilenetv2_140',\n"," 'mobilenetv3_large_100',\n"," 'mobilenetv3_large_100_miil',\n"," 'mobilenetv3_large_100_miil_in21k',\n"," 'mobilenetv3_rw',\n"," 'nasnetalarge',\n"," 'nf_regnet_b1',\n"," 'nf_resnet50',\n"," 'nfnet_l0',\n"," 'pit_b_224',\n"," 'pit_b_distilled_224',\n"," 'pit_s_224',\n"," 'pit_s_distilled_224',\n"," 'pit_ti_224',\n"," 'pit_ti_distilled_224',\n"," 'pit_xs_224',\n"," 'pit_xs_distilled_224',\n"," 'pnasnet5large',\n"," 'regnetx_002',\n"," 'regnetx_004',\n"," 'regnetx_006',\n"," 'regnetx_008',\n"," 'regnetx_016',\n"," 'regnetx_032',\n"," 'regnetx_040',\n"," 'regnetx_064',\n"," 'regnetx_080',\n"," 'regnetx_120',\n"," 'regnetx_160',\n"," 'regnetx_320',\n"," 'regnety_002',\n"," 'regnety_004',\n"," 'regnety_006',\n"," 'regnety_008',\n"," 'regnety_016',\n"," 'regnety_032',\n"," 'regnety_040',\n"," 'regnety_064',\n"," 'regnety_080',\n"," 'regnety_120',\n"," 'regnety_160',\n"," 'regnety_320',\n"," 'repvgg_a2',\n"," 'repvgg_b0',\n"," 'repvgg_b1',\n"," 'repvgg_b1g4',\n"," 'repvgg_b2',\n"," 'repvgg_b2g4',\n"," 'repvgg_b3',\n"," 'repvgg_b3g4',\n"," 'res2net50_14w_8s',\n"," 'res2net50_26w_4s',\n"," 'res2net50_26w_6s',\n"," 'res2net50_26w_8s',\n"," 'res2net50_48w_2s',\n"," 'res2net101_26w_4s',\n"," 'res2next50',\n"," 'resmlp_12_224',\n"," 'resmlp_12_distilled_224',\n"," 'resmlp_24_224',\n"," 'resmlp_24_distilled_224',\n"," 'resmlp_36_224',\n"," 'resmlp_36_distilled_224',\n"," 'resmlp_big_24_224',\n"," 'resmlp_big_24_224_in22ft1k',\n"," 'resmlp_big_24_distilled_224',\n"," 'resnest14d',\n"," 'resnest26d',\n"," 'resnest50d',\n"," 'resnest50d_1s4x24d',\n"," 'resnest50d_4s2x40d',\n"," 'resnest101e',\n"," 'resnest200e',\n"," 'resnest269e',\n"," 'resnet18',\n"," 'resnet18d',\n"," 'resnet26',\n"," 'resnet26d',\n"," 'resnet34',\n"," 'resnet34d',\n"," 'resnet50',\n"," 'resnet50d',\n"," 'resnet51q',\n"," 'resnet101d',\n"," 'resnet152d',\n"," 'resnet200d',\n"," 'resnetblur50',\n"," 'resnetrs50',\n"," 'resnetrs101',\n"," 'resnetrs152',\n"," 'resnetrs200',\n"," 'resnetrs270',\n"," 'resnetrs350',\n"," 'resnetrs420',\n"," 'resnetv2_50x1_bit_distilled',\n"," 'resnetv2_50x1_bitm',\n"," 'resnetv2_50x1_bitm_in21k',\n"," 'resnetv2_50x3_bitm',\n"," 'resnetv2_50x3_bitm_in21k',\n"," 'resnetv2_101x1_bitm',\n"," 'resnetv2_101x1_bitm_in21k',\n"," 'resnetv2_101x3_bitm',\n"," 'resnetv2_101x3_bitm_in21k',\n"," 'resnetv2_152x2_bit_teacher',\n"," 'resnetv2_152x2_bit_teacher_384',\n"," 'resnetv2_152x2_bitm',\n"," 'resnetv2_152x2_bitm_in21k',\n"," 'resnetv2_152x4_bitm',\n"," 'resnetv2_152x4_bitm_in21k',\n"," 'resnext50_32x4d',\n"," 'resnext50d_32x4d',\n"," 'resnext101_32x8d',\n"," 'rexnet_100',\n"," 'rexnet_130',\n"," 'rexnet_150',\n"," 'rexnet_200',\n"," 'selecsls42b',\n"," 'selecsls60',\n"," 'selecsls60b',\n"," 'semnasnet_100',\n"," 'seresnet50',\n"," 'seresnet152d',\n"," 'seresnext26d_32x4d',\n"," 'seresnext26t_32x4d',\n"," 'seresnext50_32x4d',\n"," 'skresnet18',\n"," 'skresnet34',\n"," 'skresnext50_32x4d',\n"," 'spnasnet_100',\n"," 'ssl_resnet18',\n"," 'ssl_resnet50',\n"," 'ssl_resnext50_32x4d',\n"," 'ssl_resnext101_32x4d',\n"," 'ssl_resnext101_32x8d',\n"," 'ssl_resnext101_32x16d',\n"," 'swin_base_patch4_window7_224',\n"," 'swin_base_patch4_window7_224_in22k',\n"," 'swin_base_patch4_window12_384',\n"," 'swin_base_patch4_window12_384_in22k',\n"," 'swin_large_patch4_window7_224',\n"," 'swin_large_patch4_window7_224_in22k',\n"," 'swin_large_patch4_window12_384',\n"," 'swin_large_patch4_window12_384_in22k',\n"," 'swin_small_patch4_window7_224',\n"," 'swin_tiny_patch4_window7_224',\n"," 'swsl_resnet18',\n"," 'swsl_resnet50',\n"," 'swsl_resnext50_32x4d',\n"," 'swsl_resnext101_32x4d',\n"," 'swsl_resnext101_32x8d',\n"," 'swsl_resnext101_32x16d',\n"," 'tf_efficientnet_b0',\n"," 'tf_efficientnet_b0_ap',\n"," 'tf_efficientnet_b0_ns',\n"," 'tf_efficientnet_b1',\n"," 'tf_efficientnet_b1_ap',\n"," 'tf_efficientnet_b1_ns',\n"," 'tf_efficientnet_b2',\n"," 'tf_efficientnet_b2_ap',\n"," 'tf_efficientnet_b2_ns',\n"," 'tf_efficientnet_b3',\n"," 'tf_efficientnet_b3_ap',\n"," 'tf_efficientnet_b3_ns',\n"," 'tf_efficientnet_b4',\n"," 'tf_efficientnet_b4_ap',\n"," 'tf_efficientnet_b4_ns',\n"," 'tf_efficientnet_b5',\n"," 'tf_efficientnet_b5_ap',\n"," 'tf_efficientnet_b5_ns',\n"," 'tf_efficientnet_b6',\n"," 'tf_efficientnet_b6_ap',\n"," 'tf_efficientnet_b6_ns',\n"," 'tf_efficientnet_b7',\n"," 'tf_efficientnet_b7_ap',\n"," 'tf_efficientnet_b7_ns',\n"," 'tf_efficientnet_b8',\n"," 'tf_efficientnet_b8_ap',\n"," 'tf_efficientnet_cc_b0_4e',\n"," 'tf_efficientnet_cc_b0_8e',\n"," 'tf_efficientnet_cc_b1_8e',\n"," 'tf_efficientnet_el',\n"," 'tf_efficientnet_em',\n"," 'tf_efficientnet_es',\n"," 'tf_efficientnet_l2_ns',\n"," 'tf_efficientnet_l2_ns_475',\n"," 'tf_efficientnet_lite0',\n"," 'tf_efficientnet_lite1',\n"," 'tf_efficientnet_lite2',\n"," 'tf_efficientnet_lite3',\n"," 'tf_efficientnet_lite4',\n"," 'tf_efficientnetv2_b0',\n"," 'tf_efficientnetv2_b1',\n"," 'tf_efficientnetv2_b2',\n"," 'tf_efficientnetv2_b3',\n"," 'tf_efficientnetv2_l',\n"," 'tf_efficientnetv2_l_in21ft1k',\n"," 'tf_efficientnetv2_l_in21k',\n"," 'tf_efficientnetv2_m',\n"," 'tf_efficientnetv2_m_in21ft1k',\n"," 'tf_efficientnetv2_m_in21k',\n"," 'tf_efficientnetv2_s',\n"," 'tf_efficientnetv2_s_in21ft1k',\n"," 'tf_efficientnetv2_s_in21k',\n"," 'tf_inception_v3',\n"," 'tf_mixnet_l',\n"," 'tf_mixnet_m',\n"," 'tf_mixnet_s',\n"," 'tf_mobilenetv3_large_075',\n"," 'tf_mobilenetv3_large_100',\n"," 'tf_mobilenetv3_large_minimal_100',\n"," 'tf_mobilenetv3_small_075',\n"," 'tf_mobilenetv3_small_100',\n"," 'tf_mobilenetv3_small_minimal_100',\n"," 'tnt_s_patch16_224',\n"," 'tresnet_l',\n"," 'tresnet_l_448',\n"," 'tresnet_m',\n"," 'tresnet_m_448',\n"," 'tresnet_m_miil_in21k',\n"," 'tresnet_xl',\n"," 'tresnet_xl_448',\n"," 'tv_densenet121',\n"," 'tv_resnet34',\n"," 'tv_resnet50',\n"," 'tv_resnet101',\n"," 'tv_resnet152',\n"," 'tv_resnext50_32x4d',\n"," 'twins_pcpvt_base',\n"," 'twins_pcpvt_large',\n"," 'twins_pcpvt_small',\n"," 'twins_svt_base',\n"," 'twins_svt_large',\n"," 'twins_svt_small',\n"," 'vgg11',\n"," 'vgg11_bn',\n"," 'vgg13',\n"," 'vgg13_bn',\n"," 'vgg16',\n"," 'vgg16_bn',\n"," 'vgg19',\n"," 'vgg19_bn',\n"," 'visformer_small',\n"," 'vit_base_patch16_224',\n"," 'vit_base_patch16_224_in21k',\n"," 'vit_base_patch16_224_miil',\n"," 'vit_base_patch16_224_miil_in21k',\n"," 'vit_base_patch16_384',\n"," 'vit_base_patch32_224',\n"," 'vit_base_patch32_224_in21k',\n"," 'vit_base_patch32_384',\n"," 'vit_base_r50_s16_224_in21k',\n"," 'vit_base_r50_s16_384',\n"," 'vit_huge_patch14_224_in21k',\n"," 'vit_large_patch16_224',\n"," 'vit_large_patch16_224_in21k',\n"," 'vit_large_patch16_384',\n"," 'vit_large_patch32_224_in21k',\n"," 'vit_large_patch32_384',\n"," 'vit_large_r50_s32_224',\n"," 'vit_large_r50_s32_224_in21k',\n"," 'vit_large_r50_s32_384',\n"," 'vit_small_patch16_224',\n"," 'vit_small_patch16_224_in21k',\n"," 'vit_small_patch16_384',\n"," 'vit_small_patch32_224',\n"," 'vit_small_patch32_224_in21k',\n"," 'vit_small_patch32_384',\n"," 'vit_small_r26_s32_224',\n"," 'vit_small_r26_s32_224_in21k',\n"," 'vit_small_r26_s32_384',\n"," 'vit_tiny_patch16_224',\n"," 'vit_tiny_patch16_224_in21k',\n"," 'vit_tiny_patch16_384',\n"," 'vit_tiny_r_s16_p8_224',\n"," 'vit_tiny_r_s16_p8_224_in21k',\n"," 'vit_tiny_r_s16_p8_384',\n"," 'wide_resnet50_2',\n"," 'wide_resnet101_2',\n"," 'xception',\n"," 'xception41',\n"," 'xception65',\n"," 'xception71']\n"]}],"metadata":{"id":"MnZYn5SlUShB"}},{"cell_type":"markdown","source":["#### 다음과 같은 방법을 통해서 원하는 모델을 찾는 것도 가능합니다"],"metadata":{"id":"GuK012_QUShC"}},{"cell_type":"code","execution_count":null,"source":["import timm\n","from pprint import pprint\n","model_names = timm.list_models('*resne*t*')\n","pprint(model_names)"],"outputs":[],"metadata":{"id":"8q11NdteUShC"}},{"cell_type":"markdown","source":["## Paper with code\n"," - https://paperswithcode.com/task/image-classification\n"," - 다양한 태스크와 데이터셋에 대한 다양한 모델들의 성능을 벤치마킹해주는 웹서비스입니다.\n"," - 해당 서비스를 통해 각 모델들의 성능 비교뿐 아니라 논문과 구현 코드로 forwarding 도 가능합니다."],"metadata":{"id":"_3W3me_bUShC"}}]}